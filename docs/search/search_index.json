{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"airflow-indexima \u00b6 Versions following Semantic Versioning Overview \u00b6 Indexima Airflow integration based on pyhive. This project is used in our prod environment with success. As it a young project, take care of change, any help is welcome :) Installation \u00b6 Install this library directly into an activated virtual environment: $ pip install airflow-indexima or add it to your Poetry project: $ poetry add airflow-indexima or you could use it as an Airflow plugin Usage \u00b6 After installation, the package can imported: $ python >>> import airflow_indexima >>> airflow_indexima.__version__ See Api documentation a simple query \u00b6 from airflow_indexima.operators import IndeximaQueryRunnerOperator ... with dag: ... op = IndeximaQueryRunnerOperator( task_id = 'my-task-id', sql_query= 'DELETE FROM Client WHERE GRPD = 1', indexima_conn_id='my-indexima-connection' ) ... a load into indexima \u00b6 from airflow_indexima.operators.indexima import IndeximaLoadDataOperator ... with dag: ... op = IndeximaLoadDataOperator( task_id = 'my-task-id', indexima_conn_id='my-indexima-connection', target_table='Client', source_select_query='select * from dsi.client', truncate=True, load_path_uri='jdbc:redshift://my-private-instance.com:5439/db_client?ssl=true&user=airflow-user&password=XXXXXXXX' ) ... get load path uri from Connection \u00b6 In order to get jdbc uri from an Airflow Connection, you could use: get_redshift_load_path_uri get_postgresql_load_path_uri from module airflow_indexima.uri Both method have this profile: Callable[[str, Optional[ConnectionDecorator]], str] Example: get_postgresql_load_path_uri(connection_id='my_conn') >> 'jdbc:postgresql://my-db:5432/db_client?ssl=true&user=airflow-user&password=XXXXXXXX' Indexima Connection \u00b6 Authentication \u00b6 PyHive supported authentication mode: 'NONE': needs a username without password 'CUSTOM': needs a username and password (default mode) 'LDAP': needs a username and password 'KERBEROS': need a kerberos service name 'NOSASL': corresponds to hive.server2.authentication=NOSASL in hive-site.xml Configuration \u00b6 You could set those parameters: host (str): The host to connect to. port (int): The (TCP) port to connect to. timeout_seconds ([int]): define the socket timeout in second (default None) socket_keepalive ([bool]): enable TCP keepalive, default false. auth (str): authentication mode username ([str]): username to login password ([str]): password to login kerberos_service_name ([str]): kerberos service name host , port , username and password came from airflow Connection configuration. timeout_seconds , socket_keepalive , auth and kerberos_service_name parameters can came from: attribut on Hook/Operator class Airflow Connection in extra parameter For example: {\"auth\": \"CUSTOM\", \"timeout_seconds\": 90, \"socket_keepalive\": true} Setted attribut override airflow connection configuration. You could add a decorator function in order to post process Connection before usage. This decorator will be executed after connection configuration (see next section). customize Connection credential access \u00b6 If you use another backend to store your password (like AWS SSM), you could define a decorator and use it as a function in your dag. from airflow.models import Connection from airflow import DAG from airdlow_indexima.uri import define_load_path_factory, get_redshift_load_path_uri def my_decorator(conn:Connection) -> Connection: # conn instance will be not shared, and use only on connection request conn.password = get_ssm_parameter(param_name=f'{conn.conn_id}.{con.login}') return conn dag = DAG( dag_id='my_dag', user_defined_macros={ # we define a macro get_load_path_uri 'get_load_path_uri': define_load_path_factory( conn_id='my-redshift-connection', decorator=my_decorator, factory=get_redshift_load_path_uri) }, ... ) with dag: ... op = IndeximaLoadDataOperator( task_id = 'my-task-id', indexima_conn_id='my-indexima-connection', target_table='Client', source_select_query='select * from dsi.client', truncate=True, load_path_uri='{{ get_load_path_uri() }}' ) ... A Connection decorator must follow this type: ConnectionDecorator = Callable[[Connection], Connection] define_load_path_factory is a function which take: a connnection identifier a decorator ConnectionDecorator an uri factory UriGeneratorFactory = Callable[[str, Optional[ConnectionDecorator]], str] and return a function with no argument which can be called as a macro in dag's operator. Optional connection parameters \u00b6 On each operator you could set this member: auth (Optional[str]): authentication mode (default: {'CUSTOM'}) kerberos_service_name (Optional[str]): optional kerberos service name timeout_seconds (Optional[Union[int, datetime.timedelta]]): define the socket timeout in second (could be an int or a timedelta) socket_keepalive (Optional[bool]): enable TCP keepalive. Note: if execution_timeout is set, it will be used as default value for timeout_seconds. Production Feedback \u00b6 In production, you could have few strange behaviour like those that we have meet. \"TSocket read 0 bytes\" \u00b6 You could fine this issue https://github.com/dropbox/PyHive/issues/240 on long load query running. Try this in sequence: check your operator configuration, and set timeout_seconds member to 3600 second for example. You could have a different behaviour when running a dag with/without airflow context in docker container. if your facing a broken pipe, after 300s, and you have an AWS NLB V2 : Read again network-load-balancers , and focus on this: Elastic Load Balancing sets the idle timeout value for TCP flows to 350 seconds. You cannot modify this value. For TCP listeners, clients or targets can use TCP keepalive packets to reset the idle timeout. TCP keepalive packets are not supported for TLS listeners. We have tried for you the \"socket_keep_alive\", and it did not work at all. Our solution was to remove our NLB and use a simple dns A field on indexima master. \"utf-8\" or could not read byte ... \u00b6 Be very welcome to add { \"serialization.encoding\": \"utf-8\"} in hive_configuration member of IndeximaHook. This setting is set in IndeximaHook. init , may you override it ? Playing Airflow without Airflow Server \u00b6 When I was trying many little things and deals with hive stuff, i wrote a single script that help me a lot. Feel free to use it (or not) to set your dag by yourself: import os import datetime from airflow.hooks.base_hook import BaseHook from airflow import DAG from airflow_indexima.operators.indexima import IndeximaLoadDataOperator # here we create our Airflow Connection os.environ['AIRFLOW_CONN_INDEXIMA_ID'] = 'hive://my-user:my-password@my-server:10000/default' conn = BaseHook.get_connection('indexima_id') dag = DAG( dag_id='my_dag', default_args={ 'start_date': datetime.datetime(year=2019, month=12, day=1), 'depends_on_past': False, 'email_on_failure': False, 'email': [], }, ) with dag: load_operator = IndeximaLoadDataOperator( task_id='my_task', indexima_conn_id='indexima_id', target_table='my_table', source_select_query=( \"select * from source_table where \" \"creation_date_tms between '2019-11-30T00:00:00+00:00' and '2019-11-30T12:59:59.000999+00:00'\" ), truncate=True, truncate_sql=( \"DELETE FROM my_table WHERE \" \"creation_date_tms between '2019-11-30T00:00:00+00:00' and '2019-11-30T12:59:59.000999+00:00'\" ), load_path_uri='jdbc:postgresql://myserver:5439/db_common?user=etl_user&password=a_strong_password&ssl=true', retries=2, execution_timeout=datetime.timedelta(hours=3), sla=datetime.timedelta(hours=1, minutes=30), ) # here we run the dag load_operator.execute(context={}) del os.environ['AIRFLOW_CONN_INDEXIMA_ID'] Thanks \u00b6 Thanks to @bartosz25 for his help with hive connection details...","title":"Home"},{"location":"#airflow-indexima","text":"Versions following Semantic Versioning","title":"airflow-indexima"},{"location":"#overview","text":"Indexima Airflow integration based on pyhive. This project is used in our prod environment with success. As it a young project, take care of change, any help is welcome :)","title":"Overview"},{"location":"#installation","text":"Install this library directly into an activated virtual environment: $ pip install airflow-indexima or add it to your Poetry project: $ poetry add airflow-indexima or you could use it as an Airflow plugin","title":"Installation"},{"location":"#usage","text":"After installation, the package can imported: $ python >>> import airflow_indexima >>> airflow_indexima.__version__ See Api documentation","title":"Usage"},{"location":"#a-simple-query","text":"from airflow_indexima.operators import IndeximaQueryRunnerOperator ... with dag: ... op = IndeximaQueryRunnerOperator( task_id = 'my-task-id', sql_query= 'DELETE FROM Client WHERE GRPD = 1', indexima_conn_id='my-indexima-connection' ) ...","title":"a simple query"},{"location":"#a-load-into-indexima","text":"from airflow_indexima.operators.indexima import IndeximaLoadDataOperator ... with dag: ... op = IndeximaLoadDataOperator( task_id = 'my-task-id', indexima_conn_id='my-indexima-connection', target_table='Client', source_select_query='select * from dsi.client', truncate=True, load_path_uri='jdbc:redshift://my-private-instance.com:5439/db_client?ssl=true&user=airflow-user&password=XXXXXXXX' ) ...","title":"a load into indexima"},{"location":"#get-load-path-uri-from-connection","text":"In order to get jdbc uri from an Airflow Connection, you could use: get_redshift_load_path_uri get_postgresql_load_path_uri from module airflow_indexima.uri Both method have this profile: Callable[[str, Optional[ConnectionDecorator]], str] Example: get_postgresql_load_path_uri(connection_id='my_conn') >> 'jdbc:postgresql://my-db:5432/db_client?ssl=true&user=airflow-user&password=XXXXXXXX'","title":"get load path uri from Connection"},{"location":"#indexima-connection","text":"","title":"Indexima Connection"},{"location":"#authentication","text":"PyHive supported authentication mode: 'NONE': needs a username without password 'CUSTOM': needs a username and password (default mode) 'LDAP': needs a username and password 'KERBEROS': need a kerberos service name 'NOSASL': corresponds to hive.server2.authentication=NOSASL in hive-site.xml","title":"Authentication"},{"location":"#configuration","text":"You could set those parameters: host (str): The host to connect to. port (int): The (TCP) port to connect to. timeout_seconds ([int]): define the socket timeout in second (default None) socket_keepalive ([bool]): enable TCP keepalive, default false. auth (str): authentication mode username ([str]): username to login password ([str]): password to login kerberos_service_name ([str]): kerberos service name host , port , username and password came from airflow Connection configuration. timeout_seconds , socket_keepalive , auth and kerberos_service_name parameters can came from: attribut on Hook/Operator class Airflow Connection in extra parameter For example: {\"auth\": \"CUSTOM\", \"timeout_seconds\": 90, \"socket_keepalive\": true} Setted attribut override airflow connection configuration. You could add a decorator function in order to post process Connection before usage. This decorator will be executed after connection configuration (see next section).","title":"Configuration"},{"location":"#customize-connection-credential-access","text":"If you use another backend to store your password (like AWS SSM), you could define a decorator and use it as a function in your dag. from airflow.models import Connection from airflow import DAG from airdlow_indexima.uri import define_load_path_factory, get_redshift_load_path_uri def my_decorator(conn:Connection) -> Connection: # conn instance will be not shared, and use only on connection request conn.password = get_ssm_parameter(param_name=f'{conn.conn_id}.{con.login}') return conn dag = DAG( dag_id='my_dag', user_defined_macros={ # we define a macro get_load_path_uri 'get_load_path_uri': define_load_path_factory( conn_id='my-redshift-connection', decorator=my_decorator, factory=get_redshift_load_path_uri) }, ... ) with dag: ... op = IndeximaLoadDataOperator( task_id = 'my-task-id', indexima_conn_id='my-indexima-connection', target_table='Client', source_select_query='select * from dsi.client', truncate=True, load_path_uri='{{ get_load_path_uri() }}' ) ... A Connection decorator must follow this type: ConnectionDecorator = Callable[[Connection], Connection] define_load_path_factory is a function which take: a connnection identifier a decorator ConnectionDecorator an uri factory UriGeneratorFactory = Callable[[str, Optional[ConnectionDecorator]], str] and return a function with no argument which can be called as a macro in dag's operator.","title":"customize Connection credential access"},{"location":"#optional-connection-parameters","text":"On each operator you could set this member: auth (Optional[str]): authentication mode (default: {'CUSTOM'}) kerberos_service_name (Optional[str]): optional kerberos service name timeout_seconds (Optional[Union[int, datetime.timedelta]]): define the socket timeout in second (could be an int or a timedelta) socket_keepalive (Optional[bool]): enable TCP keepalive. Note: if execution_timeout is set, it will be used as default value for timeout_seconds.","title":"Optional connection parameters"},{"location":"#production-feedback","text":"In production, you could have few strange behaviour like those that we have meet.","title":"Production Feedback"},{"location":"#tsocket-read-0-bytes","text":"You could fine this issue https://github.com/dropbox/PyHive/issues/240 on long load query running. Try this in sequence: check your operator configuration, and set timeout_seconds member to 3600 second for example. You could have a different behaviour when running a dag with/without airflow context in docker container. if your facing a broken pipe, after 300s, and you have an AWS NLB V2 : Read again network-load-balancers , and focus on this: Elastic Load Balancing sets the idle timeout value for TCP flows to 350 seconds. You cannot modify this value. For TCP listeners, clients or targets can use TCP keepalive packets to reset the idle timeout. TCP keepalive packets are not supported for TLS listeners. We have tried for you the \"socket_keep_alive\", and it did not work at all. Our solution was to remove our NLB and use a simple dns A field on indexima master.","title":"\"TSocket read 0 bytes\""},{"location":"#utf-8-or-could-not-read-byte","text":"Be very welcome to add { \"serialization.encoding\": \"utf-8\"} in hive_configuration member of IndeximaHook. This setting is set in IndeximaHook. init , may you override it ?","title":"\"utf-8\" or could not read byte ..."},{"location":"#playing-airflow-without-airflow-server","text":"When I was trying many little things and deals with hive stuff, i wrote a single script that help me a lot. Feel free to use it (or not) to set your dag by yourself: import os import datetime from airflow.hooks.base_hook import BaseHook from airflow import DAG from airflow_indexima.operators.indexima import IndeximaLoadDataOperator # here we create our Airflow Connection os.environ['AIRFLOW_CONN_INDEXIMA_ID'] = 'hive://my-user:my-password@my-server:10000/default' conn = BaseHook.get_connection('indexima_id') dag = DAG( dag_id='my_dag', default_args={ 'start_date': datetime.datetime(year=2019, month=12, day=1), 'depends_on_past': False, 'email_on_failure': False, 'email': [], }, ) with dag: load_operator = IndeximaLoadDataOperator( task_id='my_task', indexima_conn_id='indexima_id', target_table='my_table', source_select_query=( \"select * from source_table where \" \"creation_date_tms between '2019-11-30T00:00:00+00:00' and '2019-11-30T12:59:59.000999+00:00'\" ), truncate=True, truncate_sql=( \"DELETE FROM my_table WHERE \" \"creation_date_tms between '2019-11-30T00:00:00+00:00' and '2019-11-30T12:59:59.000999+00:00'\" ), load_path_uri='jdbc:postgresql://myserver:5439/db_common?user=etl_user&password=a_strong_password&ssl=true', retries=2, execution_timeout=datetime.timedelta(hours=3), sla=datetime.timedelta(hours=1, minutes=30), ) # here we run the dag load_operator.execute(context={}) del os.environ['AIRFLOW_CONN_INDEXIMA_ID']","title":"Playing Airflow without Airflow Server"},{"location":"#thanks","text":"Thanks to @bartosz25 for his help with hive connection details...","title":"Thanks"},{"location":"airflow_indexima.connection/","text":"Source: airflow_indexima/connection.py#L0 apply_hive_extra_setting \u00b6 apply_hive_extra_setting(connection, auth=None, kerberos_service_name=None, timeout_seconds=None, \\ socket_keepalive=None) Apply extra settings on hive connection. Args: connection (Connection): airflow connection auth (Optional[str]): optional authentication mode kerberos_service_name (Optional[str]): optional kerberos service name timeout_seconds (Optional[int]): optional define the socket timeout in second socket_keepalive (Optional[bool]): optional enable TCP keepalive. Returns: **** (Connection): configured airflow Connection instance extract_hive_extra_setting \u00b6 extract_hive_extra_setting(connection) Extract extra settings. Args: connection (Connection): airflow connection Returns: **** (Tuple[Optional[str], Optional[str], Optional[int], Optional[bool]]): a tuple (auth, kerberos_service_name, timeout_seconds, socket_keepalive)","title":"Connection"},{"location":"airflow_indexima.connection/#apply_hive_extra_setting","text":"apply_hive_extra_setting(connection, auth=None, kerberos_service_name=None, timeout_seconds=None, \\ socket_keepalive=None) Apply extra settings on hive connection. Args: connection (Connection): airflow connection auth (Optional[str]): optional authentication mode kerberos_service_name (Optional[str]): optional kerberos service name timeout_seconds (Optional[int]): optional define the socket timeout in second socket_keepalive (Optional[bool]): optional enable TCP keepalive. Returns: **** (Connection): configured airflow Connection instance","title":"apply_hive_extra_setting"},{"location":"airflow_indexima.connection/#extract_hive_extra_setting","text":"extract_hive_extra_setting(connection) Extract extra settings. Args: connection (Connection): airflow connection Returns: **** (Tuple[Optional[str], Optional[str], Optional[int], Optional[bool]]): a tuple (auth, kerberos_service_name, timeout_seconds, socket_keepalive)","title":"extract_hive_extra_setting"},{"location":"airflow_indexima.hive_transport/","text":"Source: airflow_indexima/hive_transport.py#L0 Global Variables \u00b6 HIVE_AUTH_MODES create_transport_socket \u00b6 create_transport_socket(host, port, timeout_seconds=None, socket_keepalive=None) Create a transport socket. This function expose TSocket configuration option (more for clarity rather than anything else). Args: host (str): The host to connect to. port (int): The (TCP) port to connect to. timeout_seconds (Optional[int]): define the socket timeout in second socket_keepalive (Optional[bool]): enable TCP keepalive, default False. Returns: **** (TSocket): transport socket instance. create_hive_plain_transport \u00b6 create_hive_plain_transport(socket, username, password=None) Create a TSaslClientTransport in 'PLAIN' authentication mode. Args: socket (TSocket): socket to use username (str): username to login password (Optional[str]): optional password to login Returns: **** (TSaslClientTransport): transport instance create_hive_gssapi_transport \u00b6 create_hive_gssapi_transport(socket, service_name) Create a TSaslClientTransport in 'GSSAPI' authentication mode. Args: socket (TSocket): socket to use service_name (str): kerberos service name Returns: **** (TSaslClientTransport): transport instance create_hive_nosasl_transport \u00b6 create_hive_nosasl_transport(socket) Create a TBufferedTransport in 'NOSASL' authentication mode. NOSASL corresponds to hive.server2.authentication=NOSASL in hive-site.xml Args: socket (TSocket): socket to use Returns: **** (TBufferedTransport): transport instance check_hive_connection_parameters \u00b6 check_hive_connection_parameters(auth=None, username=None, password=None, \\ kerberos_service_name=None) Check hive connection parameters. Args: auth (Optional[str]): authentication mode) username (Optional[str]): optional username to login password (Optional[str]): optional password to login kerberos_service_name (Optional[str]): optional service name Raises \u00b6 **** (ValueError): if something is wrong create_hive_transport \u00b6 create_hive_transport(host, port=None, timeout_seconds=None, socket_keepalive=None, auth=None, \\ username=None, password=None, kerberos_service_name=None) Create a TSaslClientTransport. Implementation is heavly based on pyhive.hive.Connection constructor. Args: host (str): The host to connect to. port (int): The (TCP) port to connect to. timeout_seconds (Optional[int]): define the socket timeout in second (default 60) socket_keepalive (Optional[bool]): enable TCP keepalive, default off. auth (Optional[str]): authentication mode (Defaul 'NONE') username (Optional[str]): optional username to login password (Optional[str]): optional password to login kerberos_service_name (Optional[str]): optional kerberos service name Returns: **** (TSaslClientTransport): transport instance Raises: **** (ValueError): if something is wrong","title":"Hive transport"},{"location":"airflow_indexima.hive_transport/#global-variables","text":"HIVE_AUTH_MODES","title":"Global Variables"},{"location":"airflow_indexima.hive_transport/#create_transport_socket","text":"create_transport_socket(host, port, timeout_seconds=None, socket_keepalive=None) Create a transport socket. This function expose TSocket configuration option (more for clarity rather than anything else). Args: host (str): The host to connect to. port (int): The (TCP) port to connect to. timeout_seconds (Optional[int]): define the socket timeout in second socket_keepalive (Optional[bool]): enable TCP keepalive, default False. Returns: **** (TSocket): transport socket instance.","title":"create_transport_socket"},{"location":"airflow_indexima.hive_transport/#create_hive_plain_transport","text":"create_hive_plain_transport(socket, username, password=None) Create a TSaslClientTransport in 'PLAIN' authentication mode. Args: socket (TSocket): socket to use username (str): username to login password (Optional[str]): optional password to login Returns: **** (TSaslClientTransport): transport instance","title":"create_hive_plain_transport"},{"location":"airflow_indexima.hive_transport/#create_hive_gssapi_transport","text":"create_hive_gssapi_transport(socket, service_name) Create a TSaslClientTransport in 'GSSAPI' authentication mode. Args: socket (TSocket): socket to use service_name (str): kerberos service name Returns: **** (TSaslClientTransport): transport instance","title":"create_hive_gssapi_transport"},{"location":"airflow_indexima.hive_transport/#create_hive_nosasl_transport","text":"create_hive_nosasl_transport(socket) Create a TBufferedTransport in 'NOSASL' authentication mode. NOSASL corresponds to hive.server2.authentication=NOSASL in hive-site.xml Args: socket (TSocket): socket to use Returns: **** (TBufferedTransport): transport instance","title":"create_hive_nosasl_transport"},{"location":"airflow_indexima.hive_transport/#check_hive_connection_parameters","text":"check_hive_connection_parameters(auth=None, username=None, password=None, \\ kerberos_service_name=None) Check hive connection parameters. Args: auth (Optional[str]): authentication mode) username (Optional[str]): optional username to login password (Optional[str]): optional password to login kerberos_service_name (Optional[str]): optional service name","title":"check_hive_connection_parameters"},{"location":"airflow_indexima.hive_transport/#raises","text":"**** (ValueError): if something is wrong","title":"Raises"},{"location":"airflow_indexima.hive_transport/#create_hive_transport","text":"create_hive_transport(host, port=None, timeout_seconds=None, socket_keepalive=None, auth=None, \\ username=None, password=None, kerberos_service_name=None) Create a TSaslClientTransport. Implementation is heavly based on pyhive.hive.Connection constructor. Args: host (str): The host to connect to. port (int): The (TCP) port to connect to. timeout_seconds (Optional[int]): define the socket timeout in second (default 60) socket_keepalive (Optional[bool]): enable TCP keepalive, default off. auth (Optional[str]): authentication mode (Defaul 'NONE') username (Optional[str]): optional username to login password (Optional[str]): optional password to login kerberos_service_name (Optional[str]): optional kerberos service name Returns: **** (TSaslClientTransport): transport instance Raises: **** (ValueError): if something is wrong","title":"create_hive_transport"},{"location":"airflow_indexima.hooks.indexima/","text":"Source: airflow_indexima/hooks/indexima.py#L0 IndeximaHook \u00b6 Indexima hook implementation. This implementation can be used as a context manager. with IndeximaHook(...) as hook: hook.run('select ...') This implementation can be customized with a connection_decorator function which must have this profile: Callable[[Connection], Connection] (alias ConnectionDecorator) In this handler you could retreive credentials from other backeng like aws ssm. IndeximaHook.hive_configuration \u00b6 Return hive configuration. Returns: **** (Dict[str, str]): A dictionary of Hive settings (functionally same as the set command) IndeximaHook.log \u00b6 IndeximaHook.logger \u00b6 IndeximaHook. __init__ \u00b6 __init__(self, indexima_conn_id, connection_decorator=None, dry_run=False, auth=None, \\ kerberos_service_name=None, timeout_seconds=None, socket_keepalive=None, *args) Create an IndeximaHook instance. Args: indexima_conn_id (str): connection identifier auth (str): pyhive authentication mode (defaults: 'CUSTOM') connection_decorator (Optional[ConnectionDecorator]) : optional function handler to post process connection parameter(default: None) dry_run (Optional[bool]): dry run mode (default: False). If true no action will be applied against datasource. timeout_seconds (Optional[Union[int, datetime.timedelta]]): define the socket timeout in second (could be an int or a timedelta) socket_keepalive (Optional[bool]): enable TCP keepalive. kerberos_service_name (Optional[str]): optional kerberos service name Per default, hive connection is set in 'utf-8': { \"serialization.encoding\": \"utf-8\"}","title":"Hooks"},{"location":"airflow_indexima.hooks.indexima/#indeximahook","text":"Indexima hook implementation. This implementation can be used as a context manager. with IndeximaHook(...) as hook: hook.run('select ...') This implementation can be customized with a connection_decorator function which must have this profile: Callable[[Connection], Connection] (alias ConnectionDecorator) In this handler you could retreive credentials from other backeng like aws ssm.","title":"IndeximaHook"},{"location":"airflow_indexima.hooks.indexima/#indeximahookhive_configuration","text":"Return hive configuration. Returns: **** (Dict[str, str]): A dictionary of Hive settings (functionally same as the set command)","title":"IndeximaHook.hive_configuration"},{"location":"airflow_indexima.hooks.indexima/#indeximahooklog","text":"","title":"IndeximaHook.log"},{"location":"airflow_indexima.hooks.indexima/#indeximahooklogger","text":"","title":"IndeximaHook.logger"},{"location":"airflow_indexima.hooks.indexima/#indeximahook__init__","text":"__init__(self, indexima_conn_id, connection_decorator=None, dry_run=False, auth=None, \\ kerberos_service_name=None, timeout_seconds=None, socket_keepalive=None, *args) Create an IndeximaHook instance. Args: indexima_conn_id (str): connection identifier auth (str): pyhive authentication mode (defaults: 'CUSTOM') connection_decorator (Optional[ConnectionDecorator]) : optional function handler to post process connection parameter(default: None) dry_run (Optional[bool]): dry run mode (default: False). If true no action will be applied against datasource. timeout_seconds (Optional[Union[int, datetime.timedelta]]): define the socket timeout in second (could be an int or a timedelta) socket_keepalive (Optional[bool]): enable TCP keepalive. kerberos_service_name (Optional[str]): optional kerberos service name Per default, hive connection is set in 'utf-8': { \"serialization.encoding\": \"utf-8\"}","title":"IndeximaHook.__init__"},{"location":"airflow_indexima.operators.indexima/","text":"Source: airflow_indexima/operators/indexima.py#L0 IndeximaHookBasedOperator \u00b6 Our base class for indexima operator. This class act as a wrapper on IndeximaHook. IndeximaHookBasedOperator.dag \u00b6 Returns the Operator's DAG if set, otherwise raises an error IndeximaHookBasedOperator.dag_id \u00b6 IndeximaHookBasedOperator.deps \u00b6 Returns the list of dependencies for the operator. These differ from execution context dependencies in that they are specific to tasks and can be extended/overridden by subclasses. IndeximaHookBasedOperator.downstream_list \u00b6 @property: list of tasks directly downstream IndeximaHookBasedOperator.downstream_task_ids \u00b6 IndeximaHookBasedOperator.log \u00b6 IndeximaHookBasedOperator.logger \u00b6 IndeximaHookBasedOperator.priority_weight_total \u00b6 IndeximaHookBasedOperator.schedule_interval \u00b6 The schedule interval of the DAG always wins over individual tasks so that tasks within a DAG always line up. The task still needs a schedule_interval as it may not be attached to a DAG. IndeximaHookBasedOperator.task_type \u00b6 IndeximaHookBasedOperator.upstream_list \u00b6 @property: list of tasks directly upstream IndeximaHookBasedOperator.upstream_task_ids \u00b6 IndeximaHookBasedOperator. __init__ \u00b6 __init__(*args) Create IndeximaHookBasedOperator instance. Args: task_id (str): task identifier indexima_conn_id (str): indexima connection identifier connection_decorator Optional[ConnectionDecorator]: optional connection decorator dry_run (Optional[bool]): dry run mode (default: False). If true no action will be applied against datasource. auth (Optional[str]): authentication mode (default: {'CUSTOM'}) kerberos_service_name (Optional[str]): optional kerberos service name timeout_seconds (Optional[Union[int, datetime.timedelta]]): define the socket timeout in second (could be an int or a timedelta) socket_keepalive (Optional[bool]): enable TCP keepalive. IndeximaQueryRunnerOperator \u00b6 A simple query executor. IndeximaQueryRunnerOperator.dag \u00b6 Returns the Operator's DAG if set, otherwise raises an error IndeximaQueryRunnerOperator.dag_id \u00b6 IndeximaQueryRunnerOperator.deps \u00b6 Returns the list of dependencies for the operator. These differ from execution context dependencies in that they are specific to tasks and can be extended/overridden by subclasses. IndeximaQueryRunnerOperator.downstream_list \u00b6 @property: list of tasks directly downstream IndeximaQueryRunnerOperator.downstream_task_ids \u00b6 IndeximaQueryRunnerOperator.log \u00b6 IndeximaQueryRunnerOperator.logger \u00b6 IndeximaQueryRunnerOperator.priority_weight_total \u00b6 IndeximaQueryRunnerOperator.schedule_interval \u00b6 The schedule interval of the DAG always wins over individual tasks so that tasks within a DAG always line up. The task still needs a schedule_interval as it may not be attached to a DAG. IndeximaQueryRunnerOperator.task_type \u00b6 IndeximaQueryRunnerOperator.upstream_list \u00b6 @property: list of tasks directly upstream IndeximaQueryRunnerOperator.upstream_task_ids \u00b6 IndeximaQueryRunnerOperator. __init__ \u00b6 __init__(*args) Create IndeximaQueryRunnerOperator instance. Args: task_id (str): task identifier sql_query (str): query to run indexima_conn_id (str): indexima connection identifier connection_decorator Optional[ConnectionDecorator]: optional connection decorator dry_run (Optional[bool]): dry run mode (default: False). If true no action will be applied against datasource. auth (Optional[str]): authentication mode (default: {'CUSTOM'}) kerberos_service_name (Optional[str]): optional kerberos service name timeout_seconds (Optional[Union[int, datetime.timedelta]]): define the socket timeout in second (could be an int or a timedelta) socket_keepalive (Optional[bool]): enable TCP keepalive. IndeximaLoadDataOperator \u00b6 Indexima load data operator. Operations: truncate target_table (false per default) load source_select_query into target_table using redshift_user_name credential commit/rollback target_table All fields ('target_table', 'load_path_uri', 'source_select_query', 'truncate_sql', 'format_query', 'prefix_query', 'skip_lines', 'no_check', 'limit', 'locale', 'pause_delay_in_seconds_between_query' ) support airflow macro. Syntax (see https://indexima.com/support/doc/v.1.7/Load_Data/Load_Data_Inpath.html) LOAD DATA INPATH 'path_of_the_data_source' INTO TABLE my_data_space [FORMAT 'separator' / ORC / PARQUET / JSON]; [PREFIX 'value1 \\t value2 \\t ... \\t'] [QUERY \"my_SQL_Query\"] [SKIP lines] [NOCHECK] [LIMIT n_lines] [LOCALE 'FR'] IndeximaLoadDataOperator.dag \u00b6 Returns the Operator's DAG if set, otherwise raises an error IndeximaLoadDataOperator.dag_id \u00b6 IndeximaLoadDataOperator.deps \u00b6 Returns the list of dependencies for the operator. These differ from execution context dependencies in that they are specific to tasks and can be extended/overridden by subclasses. IndeximaLoadDataOperator.downstream_list \u00b6 @property: list of tasks directly downstream IndeximaLoadDataOperator.downstream_task_ids \u00b6 IndeximaLoadDataOperator.log \u00b6 IndeximaLoadDataOperator.logger \u00b6 IndeximaLoadDataOperator.priority_weight_total \u00b6 IndeximaLoadDataOperator.schedule_interval \u00b6 The schedule interval of the DAG always wins over individual tasks so that tasks within a DAG always line up. The task still needs a schedule_interval as it may not be attached to a DAG. IndeximaLoadDataOperator.task_type \u00b6 IndeximaLoadDataOperator.upstream_list \u00b6 @property: list of tasks directly upstream IndeximaLoadDataOperator.upstream_task_ids \u00b6 IndeximaLoadDataOperator. __init__ \u00b6 __init__(self, task_id, indexima_conn_id, target_table, load_path_uri, truncate=False, \\ truncate_sql=None, connection_decorator=None, source_select_query=None, format_query=None, \\ prefix_query=None, skip_lines=None, no_check=False, limit=None, locale=None, dry_run=False, \\ auth=None, kerberos_service_name=None, timeout_seconds=None, socket_keepalive=None, \\ pause_delay_in_seconds_between_query=None, *args) Create IndeximaLoadDataOperator instance. Args: task_id (str): task identifier indexima_conn_id (str): indexima connection identifier target_table (str): target table to load into load_path_uri (str): source uri truncate (bool): if true execute truncate query before load (default: False) truncate_sql (Optional[str]): truncate query (truncate table per default) connection_decorator Optional[ConnectionDecorator]: optional connection decorator source_select_query (Optional[str]): optional sql query to select data from load_path_uri format_query (Optional[str]): optional format to identify a character separator or a file format tailored for big-data ecosystems prefix_query (Optional[str]): optional prefix used to identify a subset of data to be imported. skip_lines (Optional[int]): Allows to skip headers lines when importing data for flat text files. no_check (Optional[bool]): Do not validate data type when loading data when specified (default: False) limit (Optional[int]): It will import only the first #lines on each imported files. locale (Optional[str]): The LOCALE 'country' parameter helps to convert character. dry_run (Optional[bool]): dry run mode (default: False). If true no action will be applied against datasource. auth (Optional[str]): authentication mode (default: {'CUSTOM'}) timeout_seconds (Optional[Union[int, datetime.timedelta]]): define the socket timeout in second (could be an int or a timedelta) socket_keepalive (Optional[bool]): enable TCP keepalive. kerberos_service_name (Optional[str]): optional kerberos service name pause_delay_in_seconds_between_query (Optional[int]): optional pause delay between queries truncate, load and commit. A None, zero or negative value disable the 'pause'.","title":"Operators"},{"location":"airflow_indexima.operators.indexima/#indeximahookbasedoperator","text":"Our base class for indexima operator. This class act as a wrapper on IndeximaHook.","title":"IndeximaHookBasedOperator"},{"location":"airflow_indexima.operators.indexima/#indeximahookbasedoperatordag","text":"Returns the Operator's DAG if set, otherwise raises an error","title":"IndeximaHookBasedOperator.dag"},{"location":"airflow_indexima.operators.indexima/#indeximahookbasedoperatordag_id","text":"","title":"IndeximaHookBasedOperator.dag_id"},{"location":"airflow_indexima.operators.indexima/#indeximahookbasedoperatordeps","text":"Returns the list of dependencies for the operator. These differ from execution context dependencies in that they are specific to tasks and can be extended/overridden by subclasses.","title":"IndeximaHookBasedOperator.deps"},{"location":"airflow_indexima.operators.indexima/#indeximahookbasedoperatordownstream_list","text":"@property: list of tasks directly downstream","title":"IndeximaHookBasedOperator.downstream_list"},{"location":"airflow_indexima.operators.indexima/#indeximahookbasedoperatordownstream_task_ids","text":"","title":"IndeximaHookBasedOperator.downstream_task_ids"},{"location":"airflow_indexima.operators.indexima/#indeximahookbasedoperatorlog","text":"","title":"IndeximaHookBasedOperator.log"},{"location":"airflow_indexima.operators.indexima/#indeximahookbasedoperatorlogger","text":"","title":"IndeximaHookBasedOperator.logger"},{"location":"airflow_indexima.operators.indexima/#indeximahookbasedoperatorpriority_weight_total","text":"","title":"IndeximaHookBasedOperator.priority_weight_total"},{"location":"airflow_indexima.operators.indexima/#indeximahookbasedoperatorschedule_interval","text":"The schedule interval of the DAG always wins over individual tasks so that tasks within a DAG always line up. The task still needs a schedule_interval as it may not be attached to a DAG.","title":"IndeximaHookBasedOperator.schedule_interval"},{"location":"airflow_indexima.operators.indexima/#indeximahookbasedoperatortask_type","text":"","title":"IndeximaHookBasedOperator.task_type"},{"location":"airflow_indexima.operators.indexima/#indeximahookbasedoperatorupstream_list","text":"@property: list of tasks directly upstream","title":"IndeximaHookBasedOperator.upstream_list"},{"location":"airflow_indexima.operators.indexima/#indeximahookbasedoperatorupstream_task_ids","text":"","title":"IndeximaHookBasedOperator.upstream_task_ids"},{"location":"airflow_indexima.operators.indexima/#indeximahookbasedoperator__init__","text":"__init__(*args) Create IndeximaHookBasedOperator instance. Args: task_id (str): task identifier indexima_conn_id (str): indexima connection identifier connection_decorator Optional[ConnectionDecorator]: optional connection decorator dry_run (Optional[bool]): dry run mode (default: False). If true no action will be applied against datasource. auth (Optional[str]): authentication mode (default: {'CUSTOM'}) kerberos_service_name (Optional[str]): optional kerberos service name timeout_seconds (Optional[Union[int, datetime.timedelta]]): define the socket timeout in second (could be an int or a timedelta) socket_keepalive (Optional[bool]): enable TCP keepalive.","title":"IndeximaHookBasedOperator.__init__"},{"location":"airflow_indexima.operators.indexima/#indeximaqueryrunneroperator","text":"A simple query executor.","title":"IndeximaQueryRunnerOperator"},{"location":"airflow_indexima.operators.indexima/#indeximaqueryrunneroperatordag","text":"Returns the Operator's DAG if set, otherwise raises an error","title":"IndeximaQueryRunnerOperator.dag"},{"location":"airflow_indexima.operators.indexima/#indeximaqueryrunneroperatordag_id","text":"","title":"IndeximaQueryRunnerOperator.dag_id"},{"location":"airflow_indexima.operators.indexima/#indeximaqueryrunneroperatordeps","text":"Returns the list of dependencies for the operator. These differ from execution context dependencies in that they are specific to tasks and can be extended/overridden by subclasses.","title":"IndeximaQueryRunnerOperator.deps"},{"location":"airflow_indexima.operators.indexima/#indeximaqueryrunneroperatordownstream_list","text":"@property: list of tasks directly downstream","title":"IndeximaQueryRunnerOperator.downstream_list"},{"location":"airflow_indexima.operators.indexima/#indeximaqueryrunneroperatordownstream_task_ids","text":"","title":"IndeximaQueryRunnerOperator.downstream_task_ids"},{"location":"airflow_indexima.operators.indexima/#indeximaqueryrunneroperatorlog","text":"","title":"IndeximaQueryRunnerOperator.log"},{"location":"airflow_indexima.operators.indexima/#indeximaqueryrunneroperatorlogger","text":"","title":"IndeximaQueryRunnerOperator.logger"},{"location":"airflow_indexima.operators.indexima/#indeximaqueryrunneroperatorpriority_weight_total","text":"","title":"IndeximaQueryRunnerOperator.priority_weight_total"},{"location":"airflow_indexima.operators.indexima/#indeximaqueryrunneroperatorschedule_interval","text":"The schedule interval of the DAG always wins over individual tasks so that tasks within a DAG always line up. The task still needs a schedule_interval as it may not be attached to a DAG.","title":"IndeximaQueryRunnerOperator.schedule_interval"},{"location":"airflow_indexima.operators.indexima/#indeximaqueryrunneroperatortask_type","text":"","title":"IndeximaQueryRunnerOperator.task_type"},{"location":"airflow_indexima.operators.indexima/#indeximaqueryrunneroperatorupstream_list","text":"@property: list of tasks directly upstream","title":"IndeximaQueryRunnerOperator.upstream_list"},{"location":"airflow_indexima.operators.indexima/#indeximaqueryrunneroperatorupstream_task_ids","text":"","title":"IndeximaQueryRunnerOperator.upstream_task_ids"},{"location":"airflow_indexima.operators.indexima/#indeximaqueryrunneroperator__init__","text":"__init__(*args) Create IndeximaQueryRunnerOperator instance. Args: task_id (str): task identifier sql_query (str): query to run indexima_conn_id (str): indexima connection identifier connection_decorator Optional[ConnectionDecorator]: optional connection decorator dry_run (Optional[bool]): dry run mode (default: False). If true no action will be applied against datasource. auth (Optional[str]): authentication mode (default: {'CUSTOM'}) kerberos_service_name (Optional[str]): optional kerberos service name timeout_seconds (Optional[Union[int, datetime.timedelta]]): define the socket timeout in second (could be an int or a timedelta) socket_keepalive (Optional[bool]): enable TCP keepalive.","title":"IndeximaQueryRunnerOperator.__init__"},{"location":"airflow_indexima.operators.indexima/#indeximaloaddataoperator","text":"Indexima load data operator. Operations: truncate target_table (false per default) load source_select_query into target_table using redshift_user_name credential commit/rollback target_table All fields ('target_table', 'load_path_uri', 'source_select_query', 'truncate_sql', 'format_query', 'prefix_query', 'skip_lines', 'no_check', 'limit', 'locale', 'pause_delay_in_seconds_between_query' ) support airflow macro. Syntax (see https://indexima.com/support/doc/v.1.7/Load_Data/Load_Data_Inpath.html) LOAD DATA INPATH 'path_of_the_data_source' INTO TABLE my_data_space [FORMAT 'separator' / ORC / PARQUET / JSON]; [PREFIX 'value1 \\t value2 \\t ... \\t'] [QUERY \"my_SQL_Query\"] [SKIP lines] [NOCHECK] [LIMIT n_lines] [LOCALE 'FR']","title":"IndeximaLoadDataOperator"},{"location":"airflow_indexima.operators.indexima/#indeximaloaddataoperatordag","text":"Returns the Operator's DAG if set, otherwise raises an error","title":"IndeximaLoadDataOperator.dag"},{"location":"airflow_indexima.operators.indexima/#indeximaloaddataoperatordag_id","text":"","title":"IndeximaLoadDataOperator.dag_id"},{"location":"airflow_indexima.operators.indexima/#indeximaloaddataoperatordeps","text":"Returns the list of dependencies for the operator. These differ from execution context dependencies in that they are specific to tasks and can be extended/overridden by subclasses.","title":"IndeximaLoadDataOperator.deps"},{"location":"airflow_indexima.operators.indexima/#indeximaloaddataoperatordownstream_list","text":"@property: list of tasks directly downstream","title":"IndeximaLoadDataOperator.downstream_list"},{"location":"airflow_indexima.operators.indexima/#indeximaloaddataoperatordownstream_task_ids","text":"","title":"IndeximaLoadDataOperator.downstream_task_ids"},{"location":"airflow_indexima.operators.indexima/#indeximaloaddataoperatorlog","text":"","title":"IndeximaLoadDataOperator.log"},{"location":"airflow_indexima.operators.indexima/#indeximaloaddataoperatorlogger","text":"","title":"IndeximaLoadDataOperator.logger"},{"location":"airflow_indexima.operators.indexima/#indeximaloaddataoperatorpriority_weight_total","text":"","title":"IndeximaLoadDataOperator.priority_weight_total"},{"location":"airflow_indexima.operators.indexima/#indeximaloaddataoperatorschedule_interval","text":"The schedule interval of the DAG always wins over individual tasks so that tasks within a DAG always line up. The task still needs a schedule_interval as it may not be attached to a DAG.","title":"IndeximaLoadDataOperator.schedule_interval"},{"location":"airflow_indexima.operators.indexima/#indeximaloaddataoperatortask_type","text":"","title":"IndeximaLoadDataOperator.task_type"},{"location":"airflow_indexima.operators.indexima/#indeximaloaddataoperatorupstream_list","text":"@property: list of tasks directly upstream","title":"IndeximaLoadDataOperator.upstream_list"},{"location":"airflow_indexima.operators.indexima/#indeximaloaddataoperatorupstream_task_ids","text":"","title":"IndeximaLoadDataOperator.upstream_task_ids"},{"location":"airflow_indexima.operators.indexima/#indeximaloaddataoperator__init__","text":"__init__(self, task_id, indexima_conn_id, target_table, load_path_uri, truncate=False, \\ truncate_sql=None, connection_decorator=None, source_select_query=None, format_query=None, \\ prefix_query=None, skip_lines=None, no_check=False, limit=None, locale=None, dry_run=False, \\ auth=None, kerberos_service_name=None, timeout_seconds=None, socket_keepalive=None, \\ pause_delay_in_seconds_between_query=None, *args) Create IndeximaLoadDataOperator instance. Args: task_id (str): task identifier indexima_conn_id (str): indexima connection identifier target_table (str): target table to load into load_path_uri (str): source uri truncate (bool): if true execute truncate query before load (default: False) truncate_sql (Optional[str]): truncate query (truncate table per default) connection_decorator Optional[ConnectionDecorator]: optional connection decorator source_select_query (Optional[str]): optional sql query to select data from load_path_uri format_query (Optional[str]): optional format to identify a character separator or a file format tailored for big-data ecosystems prefix_query (Optional[str]): optional prefix used to identify a subset of data to be imported. skip_lines (Optional[int]): Allows to skip headers lines when importing data for flat text files. no_check (Optional[bool]): Do not validate data type when loading data when specified (default: False) limit (Optional[int]): It will import only the first #lines on each imported files. locale (Optional[str]): The LOCALE 'country' parameter helps to convert character. dry_run (Optional[bool]): dry run mode (default: False). If true no action will be applied against datasource. auth (Optional[str]): authentication mode (default: {'CUSTOM'}) timeout_seconds (Optional[Union[int, datetime.timedelta]]): define the socket timeout in second (could be an int or a timedelta) socket_keepalive (Optional[bool]): enable TCP keepalive. kerberos_service_name (Optional[str]): optional kerberos service name pause_delay_in_seconds_between_query (Optional[int]): optional pause delay between queries truncate, load and commit. A None, zero or negative value disable the 'pause'.","title":"IndeximaLoadDataOperator.__init__"},{"location":"airflow_indexima.uri.factory/","text":"Source: airflow_indexima/uri/factory.py#L0 define_load_path_factory \u00b6 define_load_path_factory(conn_id, decorator, factory) Create an uri factory function with UriFactory profile. Example: def my_decorator(conn:Connection) -> Connection: ... return conn decorated_redshift_uri_factory = define_load_path_factory( decorator=my_decorator, factory=get_redshift_load_path_uri ) Args: conn_id (str): connection identifier of data source decorator (ConnectionDecorator): Connection decorator factory (UriGeneratorFactory): uri decorated factory Returns: **** (UriFactory): function used as a macro to get load uri path","title":"Factory"},{"location":"airflow_indexima.uri.factory/#define_load_path_factory","text":"define_load_path_factory(conn_id, decorator, factory) Create an uri factory function with UriFactory profile. Example: def my_decorator(conn:Connection) -> Connection: ... return conn decorated_redshift_uri_factory = define_load_path_factory( decorator=my_decorator, factory=get_redshift_load_path_uri ) Args: conn_id (str): connection identifier of data source decorator (ConnectionDecorator): Connection decorator factory (UriGeneratorFactory): uri decorated factory Returns: **** (UriFactory): function used as a macro to get load uri path","title":"define_load_path_factory"},{"location":"airflow_indexima.uri.jdbc/","text":"Source: airflow_indexima/uri/jdbc.py#L0 get_jdbc_load_path_uri \u00b6 get_jdbc_load_path_uri(jdbc_type, connection_id, decorator=None) Return jdbc load path uri from a connection_id. Args: jdbc_type (str): jdbc connection type connection_id (str): source connection identifier decorator (Optional[ConnectionDecorator]): optinal connection decorator Returns: (str) load path uri get_redshift_load_path_uri \u00b6 get_redshift_load_path_uri(connection_id, decorator=None) Return redshift load path uri from a connection_id. Example: get_redshift_load_path_uri(connection_id='my_conn') >> 'jdbc:redshift://my-db:5439/db_client?ssl=true&user=airflow-user&password=XXXXXXXX' Args: connection_id (str): source connection identifier decorator (Optional[ConnectionDecorator]): optinal connection decorator Returns: (str) load path uri get_postgresql_load_path_uri \u00b6 get_postgresql_load_path_uri(connection_id, decorator=None) Return postgresql load path uri from a connection_id. Example: get_postgresql_load_path_uri(connection_id='my_conn') >> 'jdbc:postgresql://my-db:5432/db_client?ssl=true&user=airflow-user&password=XXXXXXXX' Args: connection_id (str): source connection identifier decorator (Optional[ConnectionDecorator]): optinal connection decorator Returns: (str) load path uri","title":"Jdbc"},{"location":"airflow_indexima.uri.jdbc/#get_jdbc_load_path_uri","text":"get_jdbc_load_path_uri(jdbc_type, connection_id, decorator=None) Return jdbc load path uri from a connection_id. Args: jdbc_type (str): jdbc connection type connection_id (str): source connection identifier decorator (Optional[ConnectionDecorator]): optinal connection decorator Returns: (str) load path uri","title":"get_jdbc_load_path_uri"},{"location":"airflow_indexima.uri.jdbc/#get_redshift_load_path_uri","text":"get_redshift_load_path_uri(connection_id, decorator=None) Return redshift load path uri from a connection_id. Example: get_redshift_load_path_uri(connection_id='my_conn') >> 'jdbc:redshift://my-db:5439/db_client?ssl=true&user=airflow-user&password=XXXXXXXX' Args: connection_id (str): source connection identifier decorator (Optional[ConnectionDecorator]): optinal connection decorator Returns: (str) load path uri","title":"get_redshift_load_path_uri"},{"location":"airflow_indexima.uri.jdbc/#get_postgresql_load_path_uri","text":"get_postgresql_load_path_uri(connection_id, decorator=None) Return postgresql load path uri from a connection_id. Example: get_postgresql_load_path_uri(connection_id='my_conn') >> 'jdbc:postgresql://my-db:5432/db_client?ssl=true&user=airflow-user&password=XXXXXXXX' Args: connection_id (str): source connection identifier decorator (Optional[ConnectionDecorator]): optinal connection decorator Returns: (str) load path uri","title":"get_postgresql_load_path_uri"},{"location":"airflow_indexima.uri/","text":"Source: airflow_indexima/uri/ init .py#L0 Global Variables \u00b6 factory jdbc","title":"Airflow indexima.uri"},{"location":"airflow_indexima.uri/#global-variables","text":"factory jdbc","title":"Global Variables"},{"location":"changelog/","text":"Change Log \u00b6 2.2.2 \u00b6 upgrade project template 2.2.1 (2019-12-17) \u00b6 fix api link on readme fix readme info add optional pause between truncate/load/commit operation 2.2.0 (2019-12-12) \u00b6 add dry_run mode on operator refactor uri module (simplier code) add jdbc uri generator for postgresql support full syntax of load path query add a bunch of test unit (not too soon...) and reactivate coverage integrate hive transport factory in order to manage socket configuration add support for authentication mode: ldap, custom, kerberos and none add more documentation use thrift 0.13.0 use a new connection on rollback log original error before rollback use information from https://www.ericlin.me/2015/07/how-to-configue-session-timeout-in-hive/ add 'hive_configuration' member to IndeximaHook change default time out to None use a single cursor instance per hook process set hive connection serialization encoding 'UTF-8' timeout can be specified with an int or a timedelta use execution_timeout member on operator to set timeout if this one is not specified 2.1.0 (2019-12-04) \u00b6 manage error return from indexima define IndeximaAirflowPlugin 2.0.6 (2019-12-03) \u00b6 fix usage of connection.extra parameter 2.0.5 (2019-12-03) \u00b6 fix indexima hook: (username/login) field access (auth) decorator applied before usage fix IndeximaLoadDataOperator field access fix redshit uri base 2.0.4 (2019-12-03) \u00b6 fix connection retrieval in get_redshift_load_path_uri fix redshit uri port 2.0.3 (2019-12-03) \u00b6 align dependencies constraint on thrift to pyhive and thrift-sasl 2.0.2 (2019-12-03) \u00b6 unlock fixed python 3.6.4 to ^3.6 2.0.1 (2019-12-03) \u00b6 fix default truncate query 2.0.0 (2019-12-03) \u00b6 escape quote in select query of RedshiftIndexima Operator initiate airflow contrib package complete docstyle introduce uri utilities expose ConnectionDecorator add more example 1.0.1 (2019-11-28) \u00b6 add example remove work in progress 1.0.0 (2019-11-27) \u00b6 initial project structure based on geronimo-iia/template-python add Hook implementation add Simple Operator add pyhive, ... configure documentation add a way to customize credentials retreival (with a prepare connection function handler)","title":"Change Log"},{"location":"changelog/#change-log","text":"","title":"Change Log"},{"location":"changelog/#222","text":"upgrade project template","title":"2.2.2"},{"location":"changelog/#221-2019-12-17","text":"fix api link on readme fix readme info add optional pause between truncate/load/commit operation","title":"2.2.1 (2019-12-17)"},{"location":"changelog/#220-2019-12-12","text":"add dry_run mode on operator refactor uri module (simplier code) add jdbc uri generator for postgresql support full syntax of load path query add a bunch of test unit (not too soon...) and reactivate coverage integrate hive transport factory in order to manage socket configuration add support for authentication mode: ldap, custom, kerberos and none add more documentation use thrift 0.13.0 use a new connection on rollback log original error before rollback use information from https://www.ericlin.me/2015/07/how-to-configue-session-timeout-in-hive/ add 'hive_configuration' member to IndeximaHook change default time out to None use a single cursor instance per hook process set hive connection serialization encoding 'UTF-8' timeout can be specified with an int or a timedelta use execution_timeout member on operator to set timeout if this one is not specified","title":"2.2.0 (2019-12-12)"},{"location":"changelog/#210-2019-12-04","text":"manage error return from indexima define IndeximaAirflowPlugin","title":"2.1.0 (2019-12-04)"},{"location":"changelog/#206-2019-12-03","text":"fix usage of connection.extra parameter","title":"2.0.6 (2019-12-03)"},{"location":"changelog/#205-2019-12-03","text":"fix indexima hook: (username/login) field access (auth) decorator applied before usage fix IndeximaLoadDataOperator field access fix redshit uri base","title":"2.0.5 (2019-12-03)"},{"location":"changelog/#204-2019-12-03","text":"fix connection retrieval in get_redshift_load_path_uri fix redshit uri port","title":"2.0.4 (2019-12-03)"},{"location":"changelog/#203-2019-12-03","text":"align dependencies constraint on thrift to pyhive and thrift-sasl","title":"2.0.3 (2019-12-03)"},{"location":"changelog/#202-2019-12-03","text":"unlock fixed python 3.6.4 to ^3.6","title":"2.0.2 (2019-12-03)"},{"location":"changelog/#201-2019-12-03","text":"fix default truncate query","title":"2.0.1 (2019-12-03)"},{"location":"changelog/#200-2019-12-03","text":"escape quote in select query of RedshiftIndexima Operator initiate airflow contrib package complete docstyle introduce uri utilities expose ConnectionDecorator add more example","title":"2.0.0 (2019-12-03)"},{"location":"changelog/#101-2019-11-28","text":"add example remove work in progress","title":"1.0.1 (2019-11-28)"},{"location":"changelog/#100-2019-11-27","text":"initial project structure based on geronimo-iia/template-python add Hook implementation add Simple Operator add pyhive, ... configure documentation add a way to customize credentials retreival (with a prepare connection function handler)","title":"1.0.0 (2019-11-27)"},{"location":"code_of_conduct/","text":"Contributor Covenant Code of Conduct \u00b6 Our Pledge \u00b6 In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. Our Standards \u00b6 Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Our Responsibilities \u00b6 Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. Scope \u00b6 This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers. Enforcement \u00b6 Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at jguibert@gmail.com. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership. Attribution \u00b6 This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Code of Conduct"},{"location":"code_of_conduct/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"code_of_conduct/#our-pledge","text":"In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.","title":"Our Pledge"},{"location":"code_of_conduct/#our-standards","text":"Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"code_of_conduct/#our-responsibilities","text":"Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.","title":"Our Responsibilities"},{"location":"code_of_conduct/#scope","text":"This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.","title":"Scope"},{"location":"code_of_conduct/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at jguibert@gmail.com. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.","title":"Enforcement"},{"location":"code_of_conduct/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Attribution"},{"location":"contributing/","text":"Contributing \u00b6 This project is based on Geronimo-iaa's Python Template . This is a cookiecutter template for a typical Python library following modern packaging conventions. It utilizes popular libraries to fully automate all development and deployment tasks. Setup \u00b6 Requirements \u00b6 You will need: Python 3.6\"+ Pyenv poetry Make with find, sed Make Installation \u00b6 A powerfull tool: macOS: $ xcode-select --install Linux: https://www.gnu.org/software/make * Windows: https://mingw.org/download/installer Pyenv Installation \u00b6 Pyenv will manage all our python version. Follow https://github.com/pyenv/pyenv#installation Python Installation \u00b6 Do: $ pyenv install 3.6 Note for MacOS 10.14 user : bash SDKROOT=/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk MACOSX_DEPLOYMENT_TARGET=10.14 pyenv install 3.7.3 Poetry Installation: https://poetry.eustace.io/docs/#installation \u00b6 Poetry will manage our dependencies and create our virtual environment for us. Confirm Installation \u00b6 To confirm these system dependencies are configured correctly: $ make doctor Development Tasks \u00b6 Manual \u00b6 Install project dependencies \u00b6 $ make install Note: - this target create a dummy file .install . The makefile rule depends on pyproject.toml and poetry.lock file - if for whatever reason, you have to force installation, just remove this .install file and execute a make install Run the tests \u00b6 $ make test Run static analysis \u00b6 $ make check Build the documentation \u00b6 $ make docs Build the package \u00b6 $ make build Ontain help \u00b6 For help: $ make help Integration With Visual Studio Code \u00b6 Even if we use fabulous tool like pyenv, poetry, ... at the end, we just want to go on, and code. So here, few detail of my installation. .bashrc ```bash # init pyenv with default python version if command -v pyenv 1>/dev/null 2>&1; then eval \"$(pyenv init -)\" fi add poetry in path \u00b6 export PATH=\"$HOME/.poetry/bin:$PATH\" Add Visual Studio Code (code) \u00b6 export PATH=\"$PATH:/Applications/Visual Studio Code.app/Contents/Resources/app/bin\" ``` poetry configuration: all is let with default How Launch Visual Studio Code within virtual environment created by poetry ? After do a make install , you have to do: bash poetry shell code . poetry shell will activate project virtual environment. Continuous Integration \u00b6 The CI server will report overall build status: $ make ci Release Tasks \u00b6 Release to PyPI: $ make publish","title":"Contributing"},{"location":"contributing/#contributing","text":"This project is based on Geronimo-iaa's Python Template . This is a cookiecutter template for a typical Python library following modern packaging conventions. It utilizes popular libraries to fully automate all development and deployment tasks.","title":"Contributing"},{"location":"contributing/#setup","text":"","title":"Setup"},{"location":"contributing/#requirements","text":"You will need: Python 3.6\"+ Pyenv poetry Make with find, sed","title":"Requirements"},{"location":"contributing/#make-installation","text":"A powerfull tool: macOS: $ xcode-select --install Linux: https://www.gnu.org/software/make * Windows: https://mingw.org/download/installer","title":"Make Installation"},{"location":"contributing/#pyenv-installation","text":"Pyenv will manage all our python version. Follow https://github.com/pyenv/pyenv#installation","title":"Pyenv Installation"},{"location":"contributing/#python-installation","text":"Do: $ pyenv install 3.6 Note for MacOS 10.14 user : bash SDKROOT=/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk MACOSX_DEPLOYMENT_TARGET=10.14 pyenv install 3.7.3","title":"Python Installation"},{"location":"contributing/#poetry-installation-httpspoetryeustaceiodocsinstallation","text":"Poetry will manage our dependencies and create our virtual environment for us.","title":"Poetry Installation: https://poetry.eustace.io/docs/#installation"},{"location":"contributing/#confirm-installation","text":"To confirm these system dependencies are configured correctly: $ make doctor","title":"Confirm Installation"},{"location":"contributing/#development-tasks","text":"","title":"Development Tasks"},{"location":"contributing/#manual","text":"","title":"Manual"},{"location":"contributing/#install-project-dependencies","text":"$ make install Note: - this target create a dummy file .install . The makefile rule depends on pyproject.toml and poetry.lock file - if for whatever reason, you have to force installation, just remove this .install file and execute a make install","title":"Install project dependencies"},{"location":"contributing/#run-the-tests","text":"$ make test","title":"Run the tests"},{"location":"contributing/#run-static-analysis","text":"$ make check","title":"Run static analysis"},{"location":"contributing/#build-the-documentation","text":"$ make docs","title":"Build the documentation"},{"location":"contributing/#build-the-package","text":"$ make build","title":"Build the package"},{"location":"contributing/#ontain-help","text":"For help: $ make help","title":"Ontain help"},{"location":"contributing/#integration-with-visual-studio-code","text":"Even if we use fabulous tool like pyenv, poetry, ... at the end, we just want to go on, and code. So here, few detail of my installation. .bashrc ```bash # init pyenv with default python version if command -v pyenv 1>/dev/null 2>&1; then eval \"$(pyenv init -)\" fi","title":"Integration With Visual Studio Code"},{"location":"contributing/#add-poetry-in-path","text":"export PATH=\"$HOME/.poetry/bin:$PATH\"","title":"add poetry in path"},{"location":"contributing/#add-visual-studio-code-code","text":"export PATH=\"$PATH:/Applications/Visual Studio Code.app/Contents/Resources/app/bin\" ``` poetry configuration: all is let with default How Launch Visual Studio Code within virtual environment created by poetry ? After do a make install , you have to do: bash poetry shell code . poetry shell will activate project virtual environment.","title":"Add Visual Studio Code (code)"},{"location":"contributing/#continuous-integration","text":"The CI server will report overall build status: $ make ci","title":"Continuous Integration"},{"location":"contributing/#release-tasks","text":"Release to PyPI: $ make publish","title":"Release Tasks"},{"location":"license/","text":"License \u00b6 The MIT License (MIT) Copyright \u00a9 2019, Jerome Guibert Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"license/#license","text":"The MIT License (MIT) Copyright \u00a9 2019, Jerome Guibert Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"}]}